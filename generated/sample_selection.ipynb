{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "007b9966-1759-442c-b14d-cb0699e97b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from os import path as osp\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49ebbfd7-a76b-4018-88ad-ff9e9c0dcfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'val'\n",
    "\n",
    "# download annotations from here and put them in nlxgpt/data :\n",
    "# https://drive.google.com/drive/folders/16sJjeEQE2o23G-GGUi870ubXzJjdRDua\n",
    "# (cf. nlxgpt/README.md -> Annotations Download -> VQA-X link)\n",
    "\n",
    "base_dir = osp.abspath('../data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2142eaa9-3723-407d-932a-fef276b3f503",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2b6485-a629-4e64-8ab4-3acbb86ade18",
   "metadata": {},
   "source": [
    "## Read Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ffee44-0f6e-463c-a53d-b58275596de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file): \n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def get_confident_answers(list_of_answers): \n",
    "    return [x for x in list_of_answers if x['answer_confidence'] == 'yes']\n",
    "\n",
    "def majority_vote(list_of_answers):\n",
    "    answers = [a['answer'] for a in list_of_answers]\n",
    "    return max(set(answers), key=answers.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11d52bd0-ee66-4140-b240-98d2d7ce18d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_json(osp.join(base_dir, f'vqaX_{split}.json')).T\n",
    "# restrict to confident answers\n",
    "annotations['confident_answers'] = annotations.answers.map(get_confident_answers)\n",
    "# determine top answer (later used to determine correctness of predictions)\n",
    "annotations['top_answer'] = annotations.confident_answers.map(majority_vote)\n",
    "# rename index column\n",
    "annotations = annotations.rename_axis('question_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5958346-2e28-4f6d-814c-dde0f84e59cb",
   "metadata": {},
   "source": [
    "## Read & Prepare Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32d2c552-faf6-4445-bd32-10082fc2f819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample_file(filename):\n",
    "    samples_df = pd.read_json(filename)\n",
    "    \n",
    "    # rename question id column & set as index\n",
    "    samples_df = samples_df.rename(columns={'image_id': 'question_id'}).set_index('question_id')\n",
    "\n",
    "    # split answers from predictions (and collect idx of entries where this doesn't work)\n",
    "    samples_df['answers_explanations'] = samples_df.caption.map(lambda x: x.split(' because '))\n",
    "    invalid_ids = samples_df[samples_df.answers_explanations.map(len) != 2].index\n",
    "    # restrict to valid samples\n",
    "    samples_df = samples_df[np.logical_not(samples_df.index.isin(invalid_ids))]\n",
    "\n",
    "    # separate columns for answers and explanations\n",
    "    samples_df['answer'] = samples_df.answers_explanations.map(lambda x: x[0])\n",
    "    samples_df['explanation'] = samples_df.answers_explanations.map(lambda x: x[1])\n",
    "    samples_df = samples_df.drop(columns=['answers_explanations'])\n",
    "    \n",
    "    return samples_df, invalid_ids.to_list()\n",
    "\n",
    "def is_correct_answer(entry, annotations):\n",
    "    answer = entry.answer\n",
    "    question_id = entry.name\n",
    "    return answer == annotations.loc[question_id].top_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6847a521-d0d6-4009-bd64-83d3f5ffa1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 entries invalid\n"
     ]
    }
   ],
   "source": [
    "color_file = f'./unf_captions_full_11_{split}.json'\n",
    "greyscale_file = f'./unf_captions_full_11_{split}_greyscale.json'\n",
    "\n",
    "# import\n",
    "clr_samples, clr_invalid = process_sample_file(color_file)\n",
    "bw_samples, bw_invalid = process_sample_file(greyscale_file)\n",
    "\n",
    "# filter out invalid\n",
    "invalid = set(clr_invalid + bw_invalid)\n",
    "print(f'{len(invalid)} entries invalid')\n",
    "clr_samples = clr_samples.drop(invalid, errors='ignore')\n",
    "bw_samples = bw_samples.drop(invalid, errors='ignore')\n",
    "# ensure idx are identical\n",
    "assert np.all(bw_samples.index == clr_samples.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de16494c-e2d8-40e7-8fc1-ed7b75c57630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clr acc: 0.75\n",
      "gs acc: 0.71\n"
     ]
    }
   ],
   "source": [
    "# determine correctness of predicted answers\n",
    "clr_samples['correct_answer'] = clr_samples.apply(lambda x: is_correct_answer(x, annotations), axis=1)\n",
    "print('clr acc:', round(sum(clr_samples.correct_answer) / len(clr_samples), 2))\n",
    "\n",
    "bw_samples['correct_answer'] = bw_samples.apply(lambda x: is_correct_answer(x, annotations), axis=1)\n",
    "print('gs acc:', round(sum(bw_samples.correct_answer) / len(bw_samples), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb33a7b4-f8f9-43a3-8516-591d6eed93c2",
   "metadata": {},
   "source": [
    "# Sample Selection\n",
    "\n",
    "Selection criterion: Items where the model predicts\n",
    "1. correct answers on coloured input, and \n",
    "2. incorrect answers on black/white input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfd28078-ad2e-4c62-a2d5-fb19ee9bf5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 samples (7.21 %) match the criterion\n"
     ]
    }
   ],
   "source": [
    "# ids for samples where the color model predicts the right answers\n",
    "true_clr_ids = clr_samples[clr_samples.correct_answer == True].index\n",
    "# ids for samples where the b/w model predicts the false answers\n",
    "false_bw_ids = bw_samples[bw_samples.correct_answer == False].index\n",
    "# intersection: samples that meet the selection criterion\n",
    "match_criterion = set(true_clr_ids) & set(false_bw_ids)\n",
    "\n",
    "n_hits = len(match_criterion)\n",
    "perc_hits = round((n_hits / len(clr_samples)) * 100, 2)\n",
    "\n",
    "print(f'{n_hits} samples ({perc_hits} %) match the criterion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b865c16-6cfe-440f-82d5-96e2132c6fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = list()\n",
    "\n",
    "for question_id in sorted(match_criterion):\n",
    "    \n",
    "    clr_entry = clr_samples.loc[question_id]\n",
    "    bw_entry = bw_samples.loc[question_id]\n",
    "    ann = annotations.loc[question_id]\n",
    "    \n",
    "    out.append({\n",
    "        'question_id': question_id,\n",
    "        'image_id': ann.image_id,\n",
    "        \n",
    "        'question': ann.question,\n",
    "        'top_answer': ann.top_answer,\n",
    "        'gt_answers': ann.answers,\n",
    "        'gt_explanations': ann.explanation,\n",
    "        \n",
    "        'clr_full': clr_entry.caption, \n",
    "        'clr_answer': clr_entry.answer, \n",
    "        'clr_explanation': clr_entry.explanation,\n",
    "        \n",
    "        'bw_full': bw_entry.caption, \n",
    "        'bw_answer': bw_entry.answer, \n",
    "        'bw_explanation': bw_entry.explanation \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a4fee8f-faeb-42f5-b0b7-54e0714593f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question_id                                                  3711004\n",
       "image_id                                                        3711\n",
       "question                                 What is the cat sitting on?\n",
       "top_answer                                                       car\n",
       "gt_answers         [{'answer': 'car', 'answer_confidence': 'yes',...\n",
       "gt_explanations    [cars have flat hoods and windshields, there i...\n",
       "clr_full           car because the cat is sitting on the hood of ...\n",
       "clr_answer                                                       car\n",
       "clr_explanation              the cat is sitting on the hood of a car\n",
       "bw_full            hood because the cat is sitting on the hood of...\n",
       "bw_answer                                                       hood\n",
       "bw_explanation               the cat is sitting on the hood of a car\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "pd.Series(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9dc9a25-1eed-4f21-a702-7fccdbb42684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "with open('selected_items.json', 'w') as f:\n",
    "    json.dump(out, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0857390a-2651-4b5c-80b9-3253c5ffdbf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
